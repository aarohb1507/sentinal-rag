# Environment variables for SentinelRAG MVP
# Copy this file to .env and fill in your values
# 
# MVP Strategy:
# - Groq for LLM (fast, cheap, free-tier available)
# - sentence-transformers for embeddings (local, free)
# - No OpenAI dependencies

# Groq API Key (REQUIRED for query pipeline)
# Get free tier at: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=mixtral-8x7b-32768

# API Service
NODE_ENV=development
HOST=0.0.0.0
PORT=3000
CORS_ORIGINS=http://localhost:3001

# Database
DB_HOST=localhost
DB_PORT=5432
DB_NAME=sentinelrag
DB_USER=postgres
DB_PASSWORD=postgres

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Embeddings (Local - sentence-transformers)
# CRITICAL: Must match model in worker service
EMBEDDINGS_MODEL=all-MiniLM-L6-v2

# RAG Configuration
RAG_TOP_K_KEYWORD=20
RAG_TOP_K_VECTOR=20
RAG_HYBRID_MERGE_LIMIT=30
RAG_RERANKER_TOP_K=6

# Latency Budgets (milliseconds)
LATENCY_RETRIEVAL=200
LATENCY_RERANKING=500
LATENCY_SYNTHESIS=3000

# Chunking Configuration
CHUNKING_FIXED_CHUNK_SIZE=512
CHUNKING_FIXED_CHUNK_OVERLAP=128
CHUNKING_USE_SEMANTIC_CHUNKING=true
CHUNKING_MIN_CHUNK_SIZE=256
CHUNKING_MAX_CHUNK_SIZE=1024

# Worker Configuration
WORKER_ENV=development
WORKER_HOST=0.0.0.0
WORKER_PORT=8000

# Note: NO OPENAI_API_KEY, OPENAI_MODEL, or OPENAI_EMBEDDING_MODEL
# MVP uses Groq + local embeddings
WORKER_LOG_LEVEL=INFO
WORKER_MAX_WORKERS=4
WORKER_BATCH_SIZE=10

# Web UI
NEXT_PUBLIC_API_URL=http://localhost:3000
